# -*- coding: utf-8 -*-
"""Compare_Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1inC03PbLiAhqtIGxdcNN-aeHRJOBtYtf
"""

import matplotlib.pyplot as plt
import numpy as np
import time

from sklearn import datasets
digits = datasets.load_digits()

# MNIST 데이터 셋 출력
digits = datasets.load_digits()

_, axes = plt.subplots(nrows=1, ncols=10, figsize=(15,3))
for ax, image, label in zip(axes, digits.images, digits.target):
  ax.set_axis_off()
  ax.imshow(image, cmap=plt.cm.gray_r, interpolation="nearest")
  ax.set_title("Training: %i" %label)

# 숫자의 각 픽셀은 다음과 같이 표시됨
print(digits.images[1])

print(digits.images.shape, digits.target.shape)

from sklearn.preprocessing import StandardScaler

# X, y 설정
# X는 3차원이므로 이를 2차원으로 변경
n_samples = len(digits.images)
X = digits.images.reshape(n_samples, -1)
y = digits.target

print(X.shape, y.shape)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)

from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

names = [
    "Logistic Regression",
    "Stochastic Gradient Descent",
    "LDA",
    "Multi-layer Perceptron",
    "Linear SVM",
    "Non-linear SVM",
    "Naive Bayes"
]

classifiers = [
    LogisticRegression(multi_class="multinomial", solver="lbfgs", max_iter=5000), # soft-max 사용, converge 위해 max_iter=5000
    SGDClassifier(),
    LinearDiscriminantAnalysis(),
    MLPClassifier(solver="lbfgs"), # soft-max 사용
    SVC(kernel = "linear"),
    SVC(gamma=0.001),
    GaussianNB()
]

from sklearn import metrics

result = [[0 for col in range(len(names))] for row in range(10)]

for i in range(10):
  j = 0

  print("iteration #" + str(i))
  for name, clf in zip(names, classifiers):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    print("----- " + name + " -----")
    result[i][j] = metrics.accuracy_score(y_test, y_pred)
    print("Accuracy #", i, " : ", result[i][j])
    print()

    j = j+1

    if (i == 0):
      print(
      f"Classification report for classifier {clf}:\n"
      f"{metrics.classification_report(y_test, y_pred)}\n"
      )

      disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
      disp.figure_.suptitle("Confusion Matrix - " + name)
      print(f"Confusion matrix:\n{disp.confusion_matrix}")

      plt.show()

# 10회 계산 시 평균 accuracy
for j in range(len(names)):
  accuracy = 0

  for i in range(10):
    accuracy += result[i][j]
  
  print("Average accuracy - " + names[j] + " : " + str(accuracy / 10))